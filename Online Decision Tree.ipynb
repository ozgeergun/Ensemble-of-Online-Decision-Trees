{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kolektif Proje-Incremental DT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "import math"
      ],
      "metadata": {
        "id": "TcHdTAIcMq-L"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-z7y475YMsa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/My Drive"
      ],
      "metadata": {
        "id": "PhiL0_Q-NMd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dataset_dir = '36uci'\n",
        "dataset_names = os.listdir(os.path.join(os.getcwd(), dataset_dir))\n",
        "datasets=[]\n",
        "for i in range(0,36):\n",
        "  datasets.append('/content/drive/My Drive/36uci/'+dataset_names[i]) #read datasets"
      ],
      "metadata": {
        "id": "7xLcTEC2M6mr"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_index=23"
      ],
      "metadata": {
        "id": "IUHyXPc5NU5N"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets[dataset_index]"
      ],
      "metadata": {
        "id": "Q9eTTC6dkcuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = arff.loadarff(datasets[dataset_index])\n",
        "data = pd.DataFrame(data[0])"
      ],
      "metadata": {
        "id": "ZK3AR6rSNbf_"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "xr1p76xKwRHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Class'] = data['Class'].str.decode('utf-8') #do not add b to class while reading arff file"
      ],
      "metadata": {
        "id": "FwmIxQtBbCif"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data, test = train_test_split(data, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "_gB71ccpOMgY"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=data.sample(frac=1) #shuffle dataset to build different trees"
      ],
      "metadata": {
        "id": "1gYUJV0Ba1of"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = IncrementalDTree()"
      ],
      "metadata": {
        "id": "8eYn7AEuN0X2"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(data.shape[0]): #training with one online tree \n",
        "    print(f'\\n\\nInstance no {i}')\n",
        "    tree.fit_instance(data.iloc[[i]], data.iloc[i, -1])"
      ],
      "metadata": {
        "id": "qVCUQImxN2fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = tree.predict_all(test)"
      ],
      "metadata": {
        "id": "xc8-0o4FOVTL"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct=0\n",
        "wrong=0\n",
        "for i in range(0,len(test)):\n",
        "  if str(pred[i])==str(test.iloc[i,-1]):\n",
        "    correct+=1\n",
        "  else:\n",
        "    wrong+=1\n",
        "print(\"Accuracy: \"+str(correct/len(test))) #accuracy of one online tree"
      ],
      "metadata": {
        "id": "uNp9w_dYvKeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=[]\n",
        "for k in range (0,10):\n",
        "  data=data.sample(frac=1) #shuffle dataset to build different trees\n",
        "  tree = IncrementalDTree()\n",
        "  for i in range(data.shape[0]):\n",
        "    print(f'\\n\\nInstance no {i}')\n",
        "    tree.fit_instance(data.iloc[[i]], data.iloc[i, -1]) #fit each instance iteratively\n",
        "  pred = tree.predict_all(test) #predict test samples to measure ensemble performance\n",
        "  preds.append(pred)"
      ],
      "metadata": {
        "id": "2WlfL9rXbwRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=[] \n",
        "sum_of_preds=[0]*len(test)\n",
        "\n",
        "##test evaluation;\n",
        "\n",
        "for i in range (0,10):\n",
        "  for j in range(0,len(test)):\n",
        "    sum_of_preds[j]+=int(preds[i][j])\n",
        "\n",
        "for k in range(0, len(sum_of_preds)):\n",
        "  p = sum_of_preds[k]/10\n",
        "  sum_of_preds[k] = round(p)"
      ],
      "metadata": {
        "id": "aEpggxtpbwvx"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true = 0 #calculating accuracy of the ensemble over the test data\n",
        "false = 0\n",
        "\n",
        "for i in range(0,len(test)):\n",
        "  if (int(test.iloc[i,-1])==int(sum_of_preds[i])):\n",
        "      true+=1\n",
        "  else:\n",
        "    false+=1\n",
        "\n",
        "print(\"Ensemble accuracy: \"+ str(true/(true+false)))"
      ],
      "metadata": {
        "id": "_8qFzb1hb0wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TreeNode:\n",
        "    def __init__(self, instance):\n",
        "        self.instances = instance\n",
        "        self.isleaf = True\n",
        "        self.class_labels = instance.iloc[:, -1].mode().iloc[0]\n",
        "        self.children = {}\n",
        "\n",
        "class IncrementalDTree:\n",
        "    def __init__(self):\n",
        "        self.root = None\n",
        "\n",
        "    def predict_all(self, instances): #for predicting test set\n",
        "        labels = []\n",
        "        for i in range(instances.shape[0]):\n",
        "            try:\n",
        "                labels.append(tree.predict(instances.iloc[[i]], tree.root)[0].class_labels)\n",
        "            except:\n",
        "                labels.append(tree.predict(instances.iloc[[i]], tree.root)[1].class_labels)\n",
        "        return np.array(labels)\n",
        "\n",
        "    def predict(self, instance, current_node): #predict to check if current instance is classified correctly, if not tree will be growed\n",
        "        if current_node.isleaf == True:\n",
        "            return (current_node, )\n",
        "\n",
        "        else:\n",
        "            split_attribute_val = instance[current_node.instances].iloc[0]\n",
        "            try:\n",
        "                return self.predict(instance.drop(current_node.instances, axis = 1), current_node.children[split_attribute_val])\n",
        "            except Exception as e:\n",
        "                return 'bad', current_node\n",
        "\n",
        "    def entropy(self, columns, data, target_attributes): #calculate entropy for splitting\n",
        "        frequency = {}\n",
        "        entropy_data = 0.0\n",
        "        i = 0\n",
        "        for entry in columns:\n",
        "            if (target_attributes == entry):\n",
        "                break\n",
        "            i = i + 1\n",
        "        i = i - 1\n",
        "        for entry in data:\n",
        "            if (entry[i] in frequency.keys()):\n",
        "                frequency[entry[i]] += 1.0\n",
        "            else:\n",
        "                frequency[entry[i]]  = 1.0\n",
        "        for frequency in frequency.values():\n",
        "            entropy_data += (-frequency/len(data)) * math.log(frequency/len(data), 2)\n",
        "        return entropy_data\n",
        "\n",
        "    def informationGain(self, columns, data, attr, target_attributes):  #calculate information gain for splitting\n",
        "        frequency_of_desired_var = {}\n",
        "        subset_entropy = 0.0\n",
        "        i = columns.index(attr)\n",
        "        for entry in data:\n",
        "            if (entry[i] in frequency_of_desired_var.keys()):\n",
        "                frequency_of_desired_var[entry[i]] += 1.0\n",
        "            else:\n",
        "                frequency_of_desired_var[entry[i]]  = 1.0\n",
        "\n",
        "        for val in frequency_of_desired_var.keys():\n",
        "            valProb        = frequency_of_desired_var[val] / sum(frequency_of_desired_var.values())\n",
        "            dataSubset     = [entry for entry in data if entry[i] == val]\n",
        "            subset_entropy += valProb * self.entropy(columns, dataSubset, target_attributes)\n",
        "\n",
        "        return (self.entropy(columns, data, target_attributes) - subset_entropy)               \n",
        "\n",
        "    def find_split(self, node):\n",
        "        columns = list(node.columns)\n",
        "        best = columns[0]\n",
        "        maximim_gain = 0;\n",
        "\n",
        "        for attr in columns: #for each columnn find out the information gain\n",
        "            new_info_gain = self.informationGain(columns, data.values, attr, columns[-1]) \n",
        "            if new_info_gain>maximim_gain: #if the new information gain happens to be more than the current maximum, update the current max info_gain with the new gain\n",
        "                maximim_gain = new_info_gain\n",
        "                best = attr\n",
        "\n",
        "        return best\n",
        "\n",
        "    def add_node(self, instance, current_node):\n",
        "        if current_node.isleaf == True:\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "            split_attribute_val = instance[current_node.instances].iloc[0] \n",
        "            try:\n",
        "                self.add_node(instance.drop(current_node.instances, axis = 1), current_node.children[split_attribute_val])\n",
        "            except Exception as e:\n",
        "                new_node = TreeNode(instance)\n",
        "                current_node.children[split_attribute_val] = new_node\n",
        "\n",
        "    def split(self, node):\n",
        "        split_feat = self.find_split(node.instances)\n",
        "        if split_feat == None:\n",
        "            return node\n",
        "\n",
        "        else:\n",
        "            for val in node.instances[split_feat]:\n",
        "                temp_instances = node.instances.drop(split_feat, axis = 1)\n",
        "                new_node = TreeNode(temp_instances[node.instances[split_feat] == val])\n",
        "                node.children[val] = new_node\n",
        "            node.instances = split_feat\n",
        "            node.isleaf = False\n",
        "            return node\n",
        "\n",
        "    def fit_instance(self, instance, label): #call this to fit current instance \n",
        "        if self.root == None: #first sample of dataset, build the tree\n",
        "            new_node = TreeNode(instance)\n",
        "            self.root = new_node\n",
        "\n",
        "        elif self.root != None:\n",
        "            pred_node = self.predict(instance, self.root)[0]\n",
        "            if pred_node != 'bad':\n",
        "                pred = pred_node.class_labels\n",
        "                if pred == label: #current instance is classified correctly\n",
        "                    pass\n",
        "                else:\n",
        "                    pred_node.instances = pd.concat([pred_node.instances, instance[pred_node.instances.columns]])\n",
        "                    pred_node = self.split(pred_node)\n",
        " \n",
        "            else:\n",
        "                self.add_node(instance, self.root)         "
      ],
      "metadata": {
        "id": "DDmprXOqNyIb"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}